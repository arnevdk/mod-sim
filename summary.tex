\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[dutch]{babel}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{bbold}
\usepackage{todonotes}

\title{Samenvatting [G0Q57A] - Modellering en simulatie}
\author{Arne Van Den Kerchove}

\newtheorem{mydef}{Definitie}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\setlength\parindent{0pt}

\begin{document}
	\maketitle
	
	\tableofcontents
	
	\pagebreak
	
	\section{Modellen en simulaties}
	
	\todo{Dit hoofdstuk als er tijd over is}
	
	\section{Numerieke lineaire algebra en toepassingen}
	\subsection{QR-factorisatie}
	
	
	\begin{mydef}
		De volle QR-factorisatie van de matrix $A$ wordt gegeven door
		$$	A=QR  $$
		met $q$ een $m \times m$ orthogonale matrix en $R$ een $m \times n $ bovendriehoeksmatrix.
	\end{mydef}

	\subsubsection{Gram-Schmidt orthogonalisatie}
	

	\begin{algorithm}[!ht]
		\caption{Gram-Schmidt-algoritme}
		\begin{algorithmic}[1]
			\Procedure{QRGramSchmidt}{}
				\For{$j=1$ to $n$}
					\State $v_j = a_j$
					\For{$i=1$ to $j-1$}
						\State $r_{ij} = q_i^T a_j$
						\State $v_j = v_j - r_{ij} q_i$
					\EndFor
					\State $r_{jj} = \norm{v_j}_2$
					\State $q_j = v_j/r_{jj}$
				\EndFor 
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}

	\textbf{Complexiteit:} $\mathcal{O}(2mn^2)$\\
	\textbf{Stabiliteit:} niet stabiel
	
	\subsubsection{QR-factorisatie met Givens-rotaties}
	
	\begin{mydef}
		Een Givens-rotatie is een $m \times m$ orthogonale matrix van de vorm
		$$
		G_{ij} = 
		\begin{bmatrix}
			c & -s \\
			s &  c  
		\end{bmatrix}
		$$
		met
		$$ c^2 + s^2 = 1 $$
	\end{mydef}

	Om een Givens-rotatie op te stellen die plaats $(j,k)$ $0$ maakt in matrix $A$, kies dan een element in dezelfde kolom (bv. het element boven $(j,k)$) op plaats $(i,k)$ en maak $G_{ij}$ met
	$$ c = \frac{a_{ik}}{\sqrt{a_{ik}^2 + a_{jk}^2}} \text{ en } s = \frac{a_{jk}}{\sqrt{a_{ik}^2 + a_{jk}^2}}$$
	
	\pagebreak
	
	\begin{algorithm}[!ht]
		\caption{Givens-rotatie-algoritme}
		\begin{algorithmic}[1]
			\Procedure{QRGivens}{}
				\State $Q=\mathbb{1}$
				\State $R=A$
				\For{$j=1$ to $n$}
					\For{$i=m$ to $j+1$}
						\State $c = \frac{r_{i-1,j}}{\sqrt{r_{i-1,j}^2 + r_{i,j}^2}}$
						\State $s = \frac{r_{i,j}}{\sqrt{r_{i-1,j}^2 + r_{i,j}^2}}$
						\State $r_{i,j} = 0$
						\State $r_{i-1,j} = \sqrt{r_{i-1,j}^2 + r_{i,j}^2}$
						\For{$k=j+1$ to $n$}
							\State 
							$$
								\begin{bmatrix}
									r_{i-1,k} \\
									r_{ik}
								\end{bmatrix}
								=
								\begin{bmatrix}
									c & s \\
									-s & s
								\end{bmatrix}
								\begin{bmatrix}
									r_{i-1,k} \\
									r_{i,k}
								\end{bmatrix}
							$$
						\EndFor
						\For{$k=1$ to $m$}
							\State
							$$
								\begin{bmatrix}
									q_{k,i-1} & q_{ki}
								\end{bmatrix}
								=
								\begin{bmatrix}
									q_{k,i-1} & q_{ki}
								\end{bmatrix}
								\begin{bmatrix}
									c & s \\
									-s & s
								\end{bmatrix}
							$$
						\EndFor
					\EndFor
				\EndFor
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}

	\textbf{Complexiteit:} $\mathcal{O}(3mn^2 - n^3)$ \\
	\textbf{Stabiliteit:} stabiel


	
	\subsubsection{QR-factorisatie met kolompivotering}
	Indien $A$ niet van volle rang is, is het voor de stabiliteit beter om kolompivotering toe te passen. In stap $j$ van het QR-algoritme met Givens-rotaties verwisselen we kolom $j$ met de kolom $p$ waarvan de 2-norm het grootst is.\\
		
	\textbf{Complexiteit:} $\mathcal{O}(3mn^2 - n^3)$ \\
	\textbf{Stabiliteit:} stabieler voor rang-deficiënte matrices
	
	\subsection{Singuliere-waardenontbinding}
	
	\begin{mydef}
		De singuliere-waardenontbinding van matrix $A$ wordt gegeven door
		$$
		A = \hat{U}\hat{\Sigma}V^T
		$$
		waarbij $\hat{U}$ orthonormale kolommen heeft, $\hat{\Sigma}$ een diagonaalmatrix met de singuliere waarden is en $V$ een orthogonale matrix is.
	\end{mydef}

	Eigenschappen van de SVD:
	\begin{itemize}
		\item De rang van $A$ is gelijk aan de rang van $\Sigma$ is gelijk aan het aantal niet-nul singuliere waarden.
		\item De eerste $r$ kolommen van $U$ vormen een basis voor de kolomruimte van $A$.
		\item De laatste $n-r$ kolommen van $V$ vormen een basis voor de nulruimte van $A$.
		\item $ A = U \Sigma V^T = \sum_{i=1}^{r} \sigma_i u_i v_i^T $
		\item $ norm{A}_2 = \sqrt(\sigma_1^2 + \sigma_2^2 + ... + \sigma_r^2) $
		\item De singuliere waarden zijn de vierkantswortels van de eigenwaarden van $A^TA$. De kolommen van $V$ zijn de bijhorende eigenvectoren.
		\item De singuliere waarden zijn de vierkantswortels van de $n$ grootste eigenwaarden van $AA^T$. De eerste $n$ kolommen van $U$ zijn de bijhorende eigenvectoren.
		\item Als $A$ symmetrisch is, zijn de singuliere waarden de absolute waarden van de eigenwaarden van $A$.
	\end{itemize}

	\subsubsection{Lage rangbenadering}
	
	\begin{mydef}
		De $\epsilon$-rang van een matrix $A$ wordt gedefinieerd als
		$$
			rang(A,\epsilon) = \min_{\norm{A-B}_2 \leq \epsilon} \text{rang}(B)
		$$
	\end{mydef}
	De matrix $B$ ligt $\epsilon$-dicht bij $A$ als hij een rang heeft die de kleinste is onder alle matrices die $\epsilon$-dicht bij $A$ liggen.
	
	\begin{mydef}
		Een rang k-benadering $A_k$ met $(k \leq r)$ van $A$ wordt berekend door de singuliere waardenontbinding te vermenigvuldigen, maar $\Sigma$ te vervangen door een diagonaalmatrix met de $k$ grootste singuliere waarden op de diagonaal.
	\end{mydef}

	Hierdoor geldt de eigenschap
	$$
		\norm{A-A_k}_2 = min_{B \in \mathbb{R}^{m \times m} \text{rang}(B) \leq k} \norm{A-B}_2 = \sigma_k+1
	$$
	
	
	\subsection{Kleinste-Kwadratenbenadering}
	Om de coëfficiënten te bepalen wordt een Vandermondematrix $A$ opgesteld. 
	De te minimaliseren fout bij KK-benadering wordt gegeven door
	$$
	\min_{x \in \mathbb{R}^n} \norm{b-Ax}_2 
	= \min_{x \in \mathbb{R}^n} \sqrt{\sum_{i=1}^{m}(b_i - \sum_{j=1}^{n}a_{i,j}x_j)^2}
	$$
	met $r=b-Ax$ de \textit{residuvector}.
	
	Dit probleem kan opgelost worden door $x$ te bepalen in 
	$$
	A^TAx = A^Tb
	$$
	De Vandermondematrix $A$ is slecht geconditioneerd. We zoeken dus andere manieren om het KK-probleem op te lossen.\\
	
	\subsubsection{Oplossing met QR-ontbinding}
	
	Indien de QR-factorisatie van $A$ bekend is, kan deze gebruikt worden om een oplossing voor het KK-probleem te vinden:
	$$
	\min_{x \in \mathbb{R}^n} \norm{b-Ax}_2  
	= \min_{x \in \mathbb{R}^n} \norm{b-QRx}_2
	= \min_{x \in \mathbb{R}^n} \norm{Q^Tb-RAx}_2 
	$$
	Aangezien vermenigvuldiging vooraan met een orthogonale matrix de norm behoudt. \\	
	De  vector $Q^Tb=c$ kan opgesplitst worden in  de volgende componenten:
	$
	\begin{bmatrix}
		c_1\\
		c_2
	\end{bmatrix}
	$
	met $c_1 \in \mathbb{R}^n$ en $c_2 \in \mathbb{R}^{m-n}$\\
	Hieruit volgt:
	$$
	\min_{x \in \mathbb{R}^n} \norm{b-Ax}_2 
	= 	\min_{x \in \mathbb{R}^n} \norm{
		\begin{bmatrix}
		c_1\\
		c_2
		\end{bmatrix}
		-
		\begin{bmatrix}
			\hat{R}\\
			0
		\end{bmatrix}
		x}_2 
	$$
	Volgens de stelling van Pythagoras geldt:
	$$
	\min_{x \in \mathbb{R}^n} \norm{b-Ax}_2^2
	= \min_{x \in \mathbb{R}^n} (\norm{c_1-\hat{R}x}_2^2 + \norm{c_2}_2^2)
	$$
	De vector $x$ met coëfficiënten met minimale fout kan dus ook bekomen worden als $x$ de oplossing van $\hat{R}x = c_1$\\
	
	\textbf{Complexiteit:} $\mathcal{O}(mn) + \mathcal{O}n^2$ indien QR-factorisatie bekend.\\
	\textbf{Stabiliteit:} stabiel indien $A$ van volle rang.
	

	\subsection{Eigenwaardenproblemen}
	
	\subsubsection{Methode van de machten}
	Deze methode vindt de eigenvector bij de grootste eigenwaarde en is geschikt voor ijle matrices.
	
	\begin{algorithm}[!ht]
		\caption{Methode der machten}
		\begin{algorithmic}[1]
			\Procedure{EigenPowermethod}{}
				\State $q_1$ random
				\For{$k=0,1,2,...$}
					\State take $\gamma_k$ such that $\norm{q_{k+1}}_2 = 1$
					\State $q_{k+1} = \frac{Aq_k}{\gamma_k} $
				\EndFor
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}

	$q_k$ zal convergeren naar $\gamma_1 x_1$
	
	\textbf{Voordelen:}
	\begin{itemize}
		\item Eenvoudige berekeningen
		\item zeer efficiënt voor ijle matrices
	\end{itemize}
	\textbf{Nadelen:}
	\begin{itemize}
		\item Zeer trage convergentie als $\lambda_1$ niet sterk dominant is.
	\end{itemize}

	\textbf{Convergentie:} lineair, afhankelijk van afstand tussen grootste eigenwaarden.
		
	\subsubsection{Deelruimte-iteratie}
	
	We itereren nu niet meer op één vector (methode der machten) maar op de volledige ruimte opgespannen door een orthonormaal stel vectoren.\\
	
	Voor $n$ eigenwaarden:
	
	\begin{algorithm}[!ht]
		\caption{Deelruimte-iteratie}
		\begin{algorithmic}[1]
			\Procedure{EigenPartialSpace}{}
				\State $\hat{Q}_0 = 
				\begin{bmatrix}
					q_1^0 & q_2^0 & ... & q_n^0
				\end{bmatrix}
				$ random orthonormaal
				\For{$k=0,1,2,...$}
					\State $\hat{P}_k = A\hat{Q}_{k-1}$
					\State $\hat{Q}_k \hat{R}_k = \hat{P}_k$ door QR-factorisatie
				\EndFor
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}

	\textbf{Convergentie:} lineair, afhankelijk van afstand tussen eigenwaarden.
	
	\subsubsection{QR-algoritme zonder shift}
	
	\begin{algorithm}[!ht]
		\caption{QR-algoritme zonder shift}
		\begin{algorithmic}[1]
			\Procedure{EigenQR}{}
			\For{$k=1,2,3...$}
				\State $A_k = Q_k R_k$ door QR-factorisatie
				\State $A_{k+1} = R_k Q_k$
			\EndFor
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}

	$\tilde{Q}_k = 
	\begin{bmatrix}
		\tilde{q}_1^{(k)} & \tilde{q}_2^{(k)} & ... & \tilde{q}_i^{(k)}
	\end{bmatrix}$ convergeert naar de eigenvctoren van $A$.\\
	
	
	\textbf{Complexiteit:} $\mathcal{O}(km^3)$
	
	\subsubsection{Omvorming tot Hessenbergmatrix}
	
	Het aantal stappen in het QR-algoritme kan teruggebracht worden door eerst de matrix om te vormen naar een \textit{Hessenbergvorm} m.b.v. Givens-rotaties.\\
		
	\textbf{Complexiteit van omvorming:} $\mathcal{O}(m^3)$
	
	\subsubsection{QR-algoritme met shift}
	
	De convergentie van het QR-algoritme kan versneld worden door een \textit{shift} toe te passen.
	
	\pagebreak
	
	\begin{algorithm}[!ht]
		\caption{QR-algoritme met shift}
		\begin{algorithmic}[1]
			\Procedure{EigenQRShift}{}
				\State $A=A_0$ Hessenberg
				\For{$k=1,2,3...$}
					\State $\kappa = a_{m,m}^{(k)}$
					\State $A_k - \kappa \mathbb{1} = Q_k R_k$ door QR-factorisatie
					\State $A_{k+1} = R_k Q_k + \kappa \mathbb{1}$
				\EndFor
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}	
	$\tilde{Q}_k = 
	\begin{bmatrix}
	\tilde{q}_1^{(k)} & \tilde{q}_2^{(k)} & ... & \tilde{q}_i^{(k)}
	\end{bmatrix}$ convergeert naar de eigenvctoren van $A$.\\
	
	
	\textbf{Complexiteit:} $\mathcal{O}(km^3)$ \\
	\textbf{Convergentie:} kubisch indien symmetrisch, anders lineair, afhankelijk van afstand tussen eigenwaarden.
	
	\subsection{Toepassingen in de grafentheorie}
	
	\subsubsection{PageRank}
	
	Stel een grafe op van alle links op webpagina's naar andere webpagina's. Het gewicht van de edges wordt genormaliseerd met het aantal links op de pagina die verwijst. Stel deze grafe voor als matrix $A$\\
	
	$A$ geeft ook een Markov-model voor het web.\\
	
	Gebaseerd op de matrix $A$ wordt aan elke pagina $P_i$ een score $r(P_i) \geq 0$ toegekend via de volgende principes:
	\begin{enumerate}
		\item als veel andere pagina's naar $P_i$ verwijzen, begunstigd dit $r(P_i)$
		\item als $r(P_i)$ hoog is, begunstigd dit de score van pagina's waarnaar $P_i$ verwijst.
		\item als $P_i$ weinig links heeft is dit beginstigend voor de pagina's waarnaar $P_i$ verwijst.
	\end{enumerate}

	Vervolgens wordt $A$ omgevormd tot een \textit{irreduceerbare} en \textit{rij-stochastische} matrix $\hat{A}$. Volgens deze eigenschappen heeft $\hat{A}$ $1$ als grootste eigenwaarde en kan de \textit{PageRank}-vector met scores dus gevonden worden door het volgende stelsel op te lossen:
	$$ \hat{A}^T \Pi = 1 \Pi $$
	wat neer komt op het bepalen van de eigenvector van $\hat{A}$ horende bij de dominante eigenwaarde 1.
	
	\subsubsection{Meest centrale knoop}
	
	Centraliteit van een knoop kan gemeten worden door het aantal verbindingen of lussen. Het aantal lussen van lengte $n$ vertrekkende uit knoop $i$ is $(A^n)_{i,i}$. Voor de definitie van de meest centrale knoop moeten lussen van elke lengte worden meegeteld, maar hoe groter de lengte, hoe minder gewicht er gegeven moet worden aan die lus.\\
	
	\begin{mydef}
		
		De centraliteit van een knoop $i$ in matrix $A$ wordt gegeven door $(e^A)_{i,i}$ met $e^A$ de matrix-exponentiële van matrix $A$, die berekend wordt als
		$$
		\mathbb{1} + \frac{A}{1!} + \frac{A^2}{2!} + ... + \frac{A^n}{n!} + ...
		$$
	\end{mydef}
	
	Indien de eigenwaardenontbinding van $A$ bekend is als
	$$
	 A = X diag(\lambda_1, ... , \lambda_N) X^{-1}
	$$
	kan de matrix-exponentiële berekend worden als
	$$
		e^A = X diag(e^{\lambda_1}, ... , e^{\lambda_N}) X^{-1}
	$$
	
	\section{Optimalisatie}
	
	
	
\end{document}